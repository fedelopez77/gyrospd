import abc
import torch
from gyrospd import regularizers
from gyrospd.config import DEVICE


class LossFunction(abc.ABC):

    def __init__(self, args):
        if args.regularizer_weight <= 0:
            self.regularizer = regularizers.NullRegularizer(0)
        else:
            self.regularizer = getattr(regularizers, args.regularizer)(args.regularizer_weight)

    @abc.abstractmethod
    def calculate_loss(self, model, triples):
        """
        :param model: KGModel
        :param triples: b x 3: (head, relation, tail)
        :return: Average loss within the triples batch.
        """
        pass


class NegativeSampleLoss(LossFunction, abc.ABC):
    """Abstract loss with negative sampling.
    Input batch is always of the form (head, relation, tail).
    It will run the model with positive samples of the form: (head, relation, tail) and
    with negative samples of the form: (head, relation, corrupted_tail).
    Corrupted tails are generated by taking a uniform random sample over all the entities
    """
    def __init__(self, args):
        """
        :param ini_neg_index: lower index to generate negative samples
        :param end_neg_index: higher index to generate negative samples. Pre: end_neg_index > ini_neg_index
        :param args: flags
        """
        super().__init__(args)
        self.ini_neg_index = 0
        self.end_neg_index = args.num_entities
        self.neg_sample_size = args.neg_sample_size
        self.corrupt_both = args.double_neg
        self.random_generator = torch.Generator(device=DEVICE)      # because of parallelism
        self.random_generator.manual_seed(args.seed + args.local_rank)

    def build_negative_input_batch(self, triples: torch.Tensor) -> torch.Tensor:
        """
        From a b x 3 triples tensor with (head, relation, tail) builds a
        (b * neg_sample_size) x 3 input batch of the form (head, relation, corrupted_tail)
        If self.corrupt_both is True, returns (corrupted_head, relation, corrupted_tail)
        :param triples: b * neg_sample_size x 3: (head, relation, tail)
        """
        negative_batch = triples.repeat(self.neg_sample_size, 1)
        neg_tails = self.randint(triples)
        negative_batch[:, 2] = neg_tails

        if self.corrupt_both:
            neg_heads = self.randint(triples)
            negative_batch[:, 0] = neg_heads

        return negative_batch

    def randint(self, triples):
        bs = triples.shape[0]
        return torch.randint(low=self.ini_neg_index, high=self.end_neg_index, generator=self.random_generator,
                             size=(bs * self.neg_sample_size,), dtype=triples.dtype, device=triples.device)


class BCELoss(NegativeSampleLoss):
    """
    The probability of the triplet being true is given by sigma(score(head, relation, tail))
    This loss aims to maximize the probability of positive samples and minimize the one of
    negative samples.
    """
    def __init__(self, args):
        super().__init__(args)
        self.bce = torch.nn.BCEWithLogitsLoss()

    def calculate_loss(self, model, triples):
        neg_triples = self.build_negative_input_batch(triples)
        all_triples = torch.cat((triples, neg_triples), dim=0)

        ones = torch.ones((len(triples), 1), device=triples.device)
        zeros = torch.zeros((len(neg_triples), 1), device=triples.device)
        targets = torch.cat((ones, zeros), dim=0)

        scores, dists = model(all_triples)
        loss = self.bce(scores, targets)
        loss = loss + self.regularizer(model.get_factors(triples))
        dist_to_pos = dists[:len(triples)].detach()
        dist_to_neg = dists[len(triples):].detach()
        return loss, dist_to_pos, dist_to_neg


class SeparateBCELoss(NegativeSampleLoss):
    """
    The probability of the triplet being true is given by sigma(score(head, relation, tail))
    This loss aims to maximize the probability of positive samples and minimize the one of
    negative samples.
    """
    def __init__(self, args):
        super().__init__(args)
        self.logsigmoid = torch.nn.LogSigmoid()

    def calculate_loss(self, model, triples):
        neg_triples = self.build_negative_input_batch(triples)
        all_triples = torch.cat((triples, neg_triples), dim=0)

        scores, dists = model(all_triples)
        loss_pos = -self.logsigmoid(scores[:len(triples)]).mean()
        loss_neg = -self.logsigmoid(-scores[len(triples):]).mean()
        loss = loss_pos + loss_neg + self.regularizer(model.get_factors(triples))
        dist_to_pos = dists[:len(triples)].detach()
        dist_to_neg = dists[len(triples):].detach()
        return loss, dist_to_pos, dist_to_neg


class HingeLoss(NegativeSampleLoss):
    """Hinge triple loss based on scoring positive samples higher than negative samples."""
    def __init__(self, args):
        super().__init__(args)
        self.margin = args.hinge_margin

    def calculate_loss(self, model, triples):
        neg_triples = self.build_negative_input_batch(triples)      # b * neg_sample_size x 3 = b_neg x 3
        all_triples = torch.cat((triples, neg_triples), dim=0)      # (b_pos + b_neg) x 3

        scores, dists = model(all_triples)     # (b_pos + b_neg) x 1
        bs = len(triples)
        pos_scores = scores[:bs]        # b_pos x 1
        dist_to_pos = dists[:bs].detach()
        dist_to_neg = dists[bs:].detach()

        loss = 0
        b_begin = bs
        while b_begin < len(scores):
            neg_scores = scores[b_begin:b_begin + bs]
            loss = loss + torch.sum(torch.nn.functional.relu(self.margin - pos_scores + neg_scores))
            b_begin += bs
        loss = loss / (bs * self.neg_sample_size)

        loss = loss + self.regularizer(model.get_factors(triples))
        return loss, dist_to_pos, dist_to_neg
